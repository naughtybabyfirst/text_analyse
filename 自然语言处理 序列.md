# 自然语言处理 序列模型

应用：

* 办公自动化：
* 文娱娱乐：模仿艺人聊天
* 经济：
* 法律：专利书
* 医疗：生成诊断书





提及技术

* 概率模型
  * 语言模型
  * 翻译模型
  * 文本对齐（word alignment）
  * Seq2Seq
* 相似度计算
  * 篇幅表示（feature/ embedding）
  * 编辑距离（Edit distance）
* 搜索技术
  * 关键词匹配
  * Beam search
  * Local sensitive hashing
  * 倒排索引
* Computing Device
  * 自动机
  * 规则系统
  * 分类器
* 语言技术
  * Stemming
  * 同义词识别替换
  * 中文分词
  * 语法分析
  * 语义意图理解

主要讲的：

* Seq2Seq
* 自动机
* 分类器 





自然语言处理的层次

* 词法 morphology   
  * Stemming
  * 中文分词
* 语法 synatx
  * 语法分析
* 语义 semantics
  * 语义意图理解
* 语用 progmatics
  * 文章压缩





挑战：

​	领域隔离



概率系统的工作方式



|              | 开发时间 | 对下限的影响 | 对上限的影响 |
| ------------ | -------- | ------------ | ------------ |
| 流程设计     | 5%       |              | ++           |
| 收集训练数据 | 30%      |              | +++          |
| 预处理       | 20%      | +++          |              |
| 特征抽取     | 20%      | ++           | ++           |
| 分类器       | 15%      | +            | +            |
| 预测         | 5%       |              |              |
| 评价         | 5%       |              | ++           |







* 基本分类器
* 经典序列模型
  * HMM/CRF/EM
  * 自动机
  * 语言模型
* 神经序列模型
  * 概念（LSTM）
  * TensorFlow
  * 机器翻译
  * 提速
  * 其他



一般思路：









Bash Script:

* wc/sed/awk/grep/sort/uniq/paste/cat/head/tail
  * 一个很大的txt，30s内找出出现次数最多的10个词
  * 查看第30行第40行的数据

  | 命令 | 参数 | 含义       |
  | ---- | ---- | ---------- |
  | wc   | -c   | 统计字节数 |
  |      | -l   | 统计行数   |
  |      | -m   |            |
  |      | -w   | 统计字数   |

  | 命令       | 参数 | 含义                               |
  | ---------- | ---- | ---------------------------------- |
  |            |      |                                    |
  | uniq       | -c   | 在每列旁边显示该行重复出现的次数； |
  |            | -u   | 仅显示出一次的行列；               |
  | **paste**  |      |                                    |
  | **tr命令** |      |                                    |
  |            |      |                                    |
  |            |      |                                    |

  

* Python

* Standord Core NLP

* NLTK

* TensorFlow





## 语言模型

### 概念





### N-gram语言模型

$$
P(w_1,w_2,...,w_n)
$$

链式法则


$$
P(A,B,C)=P(A)P(B|A)P(C|A,B)
$$

$$
P(w_1,w_2,...,w_n)=P(w_1)P(w_2|w_1)...P(w_n|w_1,w_2,...,w_{n-1})
$$
​	

马尔科夫假设：“无记忆性”：未来的事件，只取决于有限的历史




$$
P(w_5|w_1,w_2,w_3,w_4)
$$
unigram
$$
P(w_5)
$$
bigram
$$
P(w_5|w_4)
$$
trigram
$$
P(w_5|w_4,w_3)
$$
只取决于有限的历史
$$
P(w_1,w_2,...,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})
$$
通常写成这样
$$
P(w_1,w_2,...,w_n)=P(w_1|START)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})P(EOS|w_n)
$$


### 评价

* 外在评价
  * 准确度
* 内在评价
  * 预测测试集的能力



Perplexity：

PPL是用在自然语言处理领域（NLP）中，衡量语言模型好坏的指标。它主要是根据每个词来估计一句话出现的概率，并用句子长度作normalize，





### OOV（Out of Vocabulary）


$$
P(w_i|w_{i-1},w_{i-2})=count(w_{i-2},w_{i-1},w_i)/count(w_{i-2},w_{i-1})
$$
形似
$$
P(B|A) = P(B,A)/P(A)
$$

$$
P(王者荣耀|我喜欢) = 0   (Training 中来没有出现的词)
$$

$$
P(编程|我喜欢) = 0   (Training中没有出现的trigram)
$$

Smoothing

### 平滑方法

* A 政府给大家每个人都发一笔钱
* B 找父母要
* C 劫富济贫



* +1 平滑(A)

  * 分类问题可以用，语言模型不见效

* Back-off 回退法(B)

  * 使用Trigram，如果count(trigram)满足一定的条件

  * 否则使用Bigram

  * 否则使用Unigram   
    $$
    1/|V|
    $$
    有多少个词就是多少分之一

    

* Interpolate插值法(B)

  * 结合Trigram，Bigram，Unigram
    $$
    P_{int}(w_i|w_{i-1},w_{i-2}) = a_3P_{ML}(w_i|w_{i-1},w_{i-2}) 
    +a_2P_{ML}((w_i|w_{i-1})+a_1P_{ML}((w_i)
    $$
    其中 
    $$
    a_3 + a_2 +a_1 = 1
    $$

    $$
    P_{ML}(w_i|w_{i-1},w_{i-2}) 
    =P_{ML}((w_i|w_{i-1})=P_{ML}((w_i) =1
    $$

    

* Absolute Discount(C)

  * 

* Kneser-Ney Smoothing(C)

  * 

* Modified Kneser-Ney Smoothing(最优的方法)(C)

  * 

| 模型                          | 简单理解                               | 记住                       |
| ----------------------------- | -------------------------------------- | -------------------------- |
| +1平滑                        | 政府印钱                               | 没用                       |
| Backoff                       | 用爸爸的钱                             |                            |
| Interpolate插值法             | 自己和爸爸都出点                       | Development Set;EM         |
| Absolute Discount             | 有钱人缴固定税，<br />按爸爸的资产分配 | Leave-one-out              |
| Kneser-Ney Smoothing          | 有钱人缴固定税，<br />按爸爸的人脉分配 | 词的适配度                 |
| Modified Kneser-Ney Smoothing | 有钱人缴阶梯税，<br />按爸爸的人脉分配 | 阶梯税率。<br />最好的方法 |



### NNLM/RNNLM









### RNN 代码讲解

数据概览

* 多少行（句）

  * 

    ```bash
    wc train
    ```

* 多少个单词（token）

  * 

    ```bash
    wc train
    ```

    

* 多少种单词（type）

  * 

    ```bash
    cat train|tr ' ' '\n'|sort|uniq|wc
    ```

    

* 最长（短）的句子有多少单词

  * 

    ```bash
    awk '{print NF}'|sort -n|uniq|head
    ```

    

* 句子长度与数量关系

  * 

    ```bash
    awk '{print NF}'|sort -n|uniq -c
    ```

    



